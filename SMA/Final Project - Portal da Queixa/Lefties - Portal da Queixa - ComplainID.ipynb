{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping\n",
    "## Portal da Queixa\n",
    "### Changes may be required due to Portal da Queixa's continous updates\n",
    "\n",
    "(c) Nickolas Lago 2021 - Rev. 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages and do the initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow not verified SSL (Secure Socket Layer) certificates to be opened\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Firefox options (configurations)\n",
    "options = Options()\n",
    "\n",
    "# Add this argument to Options to hide Firefox (make it not visible)\n",
    "# options.add_argument('--headless') "
   ]
  },
  {
   "source": [
    "### Creating an empty DataFrame to store the information about each complain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame to store all complains URL to scrap it later\n",
    "complainsURL = pd.DataFrame({\"complainID\": pd.Series([], dtype=\"string\"),\n",
    "                             \"complainURL\": pd.Series([], dtype=\"string\")\n",
    "                             })"
   ]
  },
  {
   "source": [
    "## Create the function to save the URL Complains\n",
    "\n",
    "This function will go to the soup object define in the next loop and take the information about complainID and complainURL that will later be used to capture the information of the complain"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPage(soupObj, dfObj):\n",
    "\n",
    "    # Find all page complains\n",
    "    complainItemClass = \"brand-complaint-list-item-anchor\"\n",
    "    complains = soup.find_all(\"a\",{\"class\": complainItemClass})\n",
    "    itemsPerPage = len(complains)\n",
    "\n",
    "    # Loop the complains and save its url\n",
    "    for i in range(0, itemsPerPage):\n",
    "        complains = soup.find_all(\"a\",{\"class\": complainItemClass})\n",
    "        complainItem = complains[i]\n",
    "\n",
    "        # Get complain ID\n",
    "        cId = complainItem.find(\"h5\").get_text()\n",
    "\n",
    "        # Get complain link\n",
    "        cUrl = complainItem[\"href\"]\n",
    "\n",
    "        # Append values to a DataFrame\n",
    "        dfObj = dfObj.append({\"complainID\": cId,\n",
    "                                   \"complainURL\": cUrl},\n",
    "                                   ignore_index=True)\n",
    "    return dfObj"
   ]
  },
  {
   "source": [
    "## Main Loop\n",
    "This loop will take a pagesNum and take all complains identification on that page [complainID, complainUrl]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing page  1  out of  10\n",
      "Extraction  1  out of  10\n",
      "Processing page  2  out of  10\n",
      "Extraction  2  out of  10\n",
      "Processing page  3  out of  10\n",
      "Extraction  3  out of  10\n",
      "Processing page  4  out of  10\n",
      "Extraction  4  out of  10\n",
      "Processing page  5  out of  10\n",
      "Extraction  5  out of  10\n",
      "Processing page  6  out of  10\n",
      "Extraction  6  out of  10\n",
      "Processing page  7  out of  10\n",
      "Extraction  7  out of  10\n",
      "Processing page  8  out of  10\n",
      "Extraction  8  out of  10\n",
      "Processing page  9  out of  10\n",
      "Extraction  9  out of  10\n",
      "Processing page  10  out of  10\n",
      "Extraction  10  out of  10\n",
      "Processing page  11  out of  10\n",
      "Extraction  11  out of  10\n",
      "Processing page  12  out of  10\n",
      "Extraction  12  out of  10\n",
      "Processing page  13  out of  10\n",
      "Extraction  13  out of  10\n",
      "Processing page  14  out of  10\n",
      "Extraction  14  out of  10\n",
      "Processing page  15  out of  10\n",
      "Extraction  15  out of  10\n",
      "Processing page  16  out of  10\n",
      "Extraction  16  out of  10\n",
      "Processing page  17  out of  10\n",
      "Extraction  17  out of  10\n",
      "Processing page  18  out of  10\n",
      "Extraction  18  out of  10\n",
      "Processing page  19  out of  10\n",
      "Extraction  19  out of  10\n",
      "Processing page  20  out of  10\n",
      "Extraction  20  out of  10\n"
     ]
    }
   ],
   "source": [
    "# Define number of pages\n",
    "\n",
    "pagesNum = 20\n",
    "\n",
    "for i in range(0,20):\n",
    "    # i starts at 0, therefore we add one to start\n",
    "    i = i + 1\n",
    "    print(\"Processing page \", i, \" out of \", 10)\n",
    "\n",
    "    # Search for the page according to the page number\n",
    "    url = \"https://portaldaqueixa.com/brands/lefties/complaints?p=\" + str(i)\n",
    "    # Open the page\n",
    "    page = urlopen(url)\n",
    "    # Read page html\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    #Create soupObject\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    complainsURL = processPage(soup, complainsURL)\n",
    "    print(\"Extraction \", i, \" out of \", 10)\n"
   ]
  },
  {
   "source": [
    "### Save it to an excel sheet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as an excel\n",
    "complainsURL.to_excel(\"reviews_url.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0f30a98447ab0ce6ed2c42a906dd8ff7c3035168d8630597903509f729a93be92",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}